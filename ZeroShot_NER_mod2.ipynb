{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36f39d62",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://sparknlp.org/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/named-entity-recognition/ZeroShot_NER.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "212325cc-182f-4565-abed-9b46864d6d69",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with ZeroShotNer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216EshxBJ9ra",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e6c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark==3.3.0  spark-nlp==4.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc39c840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7d79bbe8-04d0-4dda-a20a-1ab6be31ad19;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.3.1 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.16.0 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.api-client#google-api-client;2.1.1 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.9.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.9.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.105.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.9.0 in central\n",
      "\tfound io.grpc#grpc-core;1.51.0 in central\n",
      "\tfound com.google.api#gax;2.20.1 in central\n",
      "\tfound com.google.api#gax-grpc;2.20.1 in central\n",
      "\tfound io.grpc#grpc-alts;1.51.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.51.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.51.0 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.13.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.13.0 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.51.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.6.22 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.10 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.10 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.11.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.51.0 in central\n",
      "\tfound io.grpc#grpc-auth;1.51.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.51.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.28.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.22 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.51.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.51.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.51.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.51.0 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.51.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "downloading https://repo1.maven.org/maven2/com/johnsnowlabs/nlp/spark-nlp_2.12/4.3.1/spark-nlp_2.12-4.3.1.jar ...\n",
      "\t[SUCCESSFUL ] com.johnsnowlabs.nlp#spark-nlp_2.12;4.3.1!spark-nlp_2.12.jar (1008ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-storage/2.16.0/google-cloud-storage-2.16.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-storage;2.16.0!google-cloud-storage.jar (15ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.16/error_prone_annotations-2.16.jar ...\n",
      "\t[SUCCESSFUL ] com.google.errorprone#error_prone_annotations;2.16!error_prone_annotations.jar (21ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client/1.42.3/google-http-client-1.42.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client;1.42.3!google-http-client.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-jackson2/1.42.3/google-http-client-jackson2-1.42.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-jackson2;1.42.3!google-http-client-jackson2.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-gson/1.42.3/google-http-client-gson-1.42.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-gson;1.42.3!google-http-client-gson.jar (5ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api-client/google-api-client/2.1.1/google-api-client-2.1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api-client#google-api-client;2.1.1!google-api-client.jar (11ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-apache-v2/1.42.3/google-http-client-apache-v2-1.42.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-apache-v2;1.42.3!google-http-client-apache-v2.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.10/gson-2.10.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.10!gson.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-core/2.9.0/google-cloud-core-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-core;2.9.0!google-cloud-core.jar (11ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-core-http/2.9.0/google-cloud-core-http-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-core-http;2.9.0!google-cloud-core-http.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/http-client/google-http-client-appengine/1.42.3/google-http-client-appengine-1.42.3.jar ...\n",
      "\t[SUCCESSFUL ] com.google.http-client#google-http-client-appengine;1.42.3!google-http-client-appengine.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/gax-httpjson/0.105.1/gax-httpjson-0.105.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#gax-httpjson;0.105.1!gax-httpjson.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/google-cloud-core-grpc/2.9.0/google-cloud-core-grpc-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud#google-cloud-core-grpc;2.9.0!google-cloud-core-grpc.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-core/1.51.0/grpc-core-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-core;1.51.0!grpc-core.jar (20ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/gax/2.20.1/gax-2.20.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#gax;2.20.1!gax.jar (12ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/gax-grpc/2.20.1/gax-grpc-2.20.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#gax-grpc;2.20.1!gax-grpc.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-alts/1.51.0/grpc-alts-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-alts;1.51.0!grpc-alts.jar (14ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-grpclb/1.51.0/grpc-grpclb-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-grpclb;1.51.0!grpc-grpclb.jar (16ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-protobuf/1.51.0/grpc-protobuf-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-protobuf;1.51.0!grpc-protobuf.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/auth/google-auth-library-credentials/1.13.0/google-auth-library-credentials-1.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.auth#google-auth-library-credentials;1.13.0!google-auth-library-credentials.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/auth/google-auth-library-oauth2-http/1.13.0/google-auth-library-oauth2-http-1.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.auth#google-auth-library-oauth2-http;1.13.0!google-auth-library-oauth2-http.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/api-common/2.2.2/api-common-2.2.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api#api-common;2.2.2!api-common.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-context/1.51.0/grpc-context-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-context;1.51.0!grpc-context.jar (4ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/proto-google-iam-v1/1.6.22/proto-google-iam-v1-1.6.22.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#proto-google-iam-v1;1.6.22!proto-google-iam-v1.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/3.21.10/protobuf-java-3.21.10.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;3.21.10!protobuf-java.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java-util/3.21.10/protobuf-java-util-3.21.10.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java-util;3.21.10!protobuf-java-util.jar(bundle) (11ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/proto-google-common-protos/2.11.0/proto-google-common-protos-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#proto-google-common-protos;2.11.0!proto-google-common-protos.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/threeten/threetenbp/1.6.4/threetenbp-1.6.4.jar ...\n",
      "\t[SUCCESSFUL ] org.threeten#threetenbp;1.6.4!threetenbp.jar (13ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/proto-google-cloud-storage-v2/2.16.0-alpha/proto-google-cloud-storage-v2-2.16.0-alpha.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha!proto-google-cloud-storage-v2.jar (18ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/grpc-google-cloud-storage-v2/2.16.0-alpha/grpc-google-cloud-storage-v2-2.16.0-alpha.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha!grpc-google-cloud-storage-v2.jar (9ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/gapic-google-cloud-storage-v2/2.16.0-alpha/gapic-google-cloud-storage-v2-2.16.0-alpha.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha!gapic-google-cloud-storage-v2.jar (8ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.14.1/jackson-core-2.14.1.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.14.1!jackson-core.jar(bundle) (14ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-api/1.51.0/grpc-api-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-api;1.51.0!grpc-api.jar (10ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-auth/1.51.0/grpc-auth-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-auth;1.51.0!grpc-auth.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-stub/1.51.0/grpc-stub-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-stub;1.51.0!grpc-stub.jar (7ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.28.0/checker-qual-3.28.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.28.0!checker-qual.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/api/grpc/grpc-google-iam-v1/1.6.22/grpc-google-iam-v1-1.6.22.jar ...\n",
      "\t[SUCCESSFUL ] com.google.api.grpc#grpc-google-iam-v1;1.6.22!grpc-google-iam-v1.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-protobuf-lite/1.51.0/grpc-protobuf-lite-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-protobuf-lite;1.51.0!grpc-protobuf-lite.jar (4ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-netty-shaded/1.51.0/grpc-netty-shaded-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-netty-shaded;1.51.0!grpc-netty-shaded.jar (174ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-googleapis/1.51.0/grpc-googleapis-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-googleapis;1.51.0!grpc-googleapis.jar (6ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-xds/1.51.0/grpc-xds-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-xds;1.51.0!grpc-xds.jar (154ms)\n",
      "downloading https://repo1.maven.org/maven2/io/grpc/grpc-services/1.51.0/grpc-services-1.51.0.jar ...\n",
      "\t[SUCCESSFUL ] io.grpc#grpc-services;1.51.0!grpc-services.jar (23ms)\n",
      ":: resolution report :: resolve 14810ms :: artifacts dl 1861ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.1 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.20.1 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.20.1 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.105.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.1.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.22 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.11.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.6.22 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.13.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.13.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.16.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.10 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.10 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.3.1 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.51.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.28.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.10] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.10] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   73  |   43  |   43  |   3   ||   70  |   43  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7d79bbe8-04d0-4dda-a20a-1ab6be31ad19\n",
      "\tconfs: [default]\n",
      "\t43 artifacts copied, 27 already retrieved (75773kB/218ms)\n",
      "23/12/05 18:10:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  4.3.1\n",
      "Apache Spark version:  3.5.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-22-112.eu-central-1.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc4132df040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32aee6",
   "metadata": {},
   "source": [
    "# Zero-shot Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43420eee-1c29-4148-b1c8-fa7884eff9b3",
   "metadata": {},
   "source": [
    "`Zero-shot` is a new inference paradigm which allows us to use a model for prediction without any previous training step.\n",
    "\n",
    "For doing that, several examples (_hypotheses_) are provided and sent to the Language model, which will use `NLI (Natural Language Inference)` to check if the any information found in the text matches the examples (confirm the hypotheses).\n",
    "\n",
    "NLI usually works by trying to _confirm or reject an hypotheses_. The _hypotheses_ are the `prompts` or examples we are going to provide. If any piece of information confirm the constructed hypotheses (answer the examples we are given), then the hypotheses is confirmed and the Zero-shot is triggered.\n",
    "\n",
    "Let's see it  in action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953991cb-8efc-40e6-aa8d-dade670b36b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_base_qa_squad2 download started this may take some time.\n",
      "Approximate size to download 442.6 MB\n",
      "[ | ]roberta_base_qa_squad2 download started this may take some time.\n",
      "Approximate size to download 442.6 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 18:12:47.634549: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "sen = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "sparktokenizer = Tokenizer()\\\n",
    "  .setInputCols(\"sentence\")\\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "zero_shot_ner = ZeroShotNerModel.pretrained(\"roberta_base_qa_squad2\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"zero_shot_ner\")\\\n",
    "    .setEntityDefinitions(\n",
    "        {\n",
    "            \"DATE\": ['When was the company acquisition?', 'When was the company purchase agreement?'],\n",
    "            \"ORG\": [\"Which company was acquired?\"],\n",
    "            \"PRODUCT\": [\"Which product?\"],\n",
    "            \"PROFIT_INCREASE\": [\"How much has the gross profit increased?\"],\n",
    "            \"REVENUES_DECLINED\": [\"How much has the revenues declined?\"],\n",
    "            \"OPERATING_LOSS_2020\": [\"Which was the operating loss in 2020\"],\n",
    "            \"OPERATING_LOSS_2019\": [\"Which was the operating loss in 2019\"]\n",
    "        })\n",
    "\n",
    "nerconverter = NerConverter()\\\n",
    "  .setInputCols([\"sentence\", \"token\", \"zero_shot_ner\"])\\\n",
    "  .setOutputCol(\"ner_chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "071cc2b5-43cd-42fd-b32d-7bad894ded13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline =  Pipeline(stages=[\n",
    "  documentAssembler,\n",
    "  sen,\n",
    "  sparktokenizer,\n",
    "  zero_shot_ner,\n",
    "  nerconverter\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06e991d4-d11b-4ef0-8bca-f6b2ba8ac064",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "sample_text = [\"In March 2012, as part of a longer-term strategy, the Company acquired Vertro, Inc., which owned and operated the ALOT product portfolio.\",\n",
    "              \"In February 2017, the Company entered into an asset purchase agreement with NetSeer, Inc.\",\n",
    "              \"While our gross profit margin increased to 81.4% in 2020 from 63.1% in 2019, our revenues declined approximately 27% in 2020 as compared to 2019.\",\n",
    "              \"We reported an operating loss of approximately $8,048,581 million in 2020 as compared to an operating loss of $7,738,193 in 2019.\"]\n",
    "\n",
    "p_model = pipeline.fit(spark.createDataFrame([[\"\"]], StringType()).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a9f444-ed1d-490b-a6bd-c241b2e7603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(sample_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aedd4aaf-cf5f-42a1-a4c3-4dd449a7a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|In March 2012, as...|\n",
      "|In February 2017,...|\n",
      "|While our gross p...|\n",
      "|We reported an op...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94405bca-6b32-432f-81e9-24da23369b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.transform.\n: java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(Lorg/apache/spark/sql/types/StructType;)Lorg/apache/spark/sql/catalyst/encoders/ExpressionEncoder;\n\tat com.johnsnowlabs.nlp.AnnotatorModel._transform(AnnotatorModel.scala:68)\n\tat com.johnsnowlabs.nlp.AnnotatorModel.transform(AnnotatorModel.scala:129)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/pipeline.py:304\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[0;32m--> 304\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:396\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m-> 1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.transform.\n: java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.encoders.RowEncoder$.apply(Lorg/apache/spark/sql/types/StructType;)Lorg/apache/spark/sql/catalyst/encoders/ExpressionEncoder;\n\tat com.johnsnowlabs.nlp.AnnotatorModel._transform(AnnotatorModel.scala:68)\n\tat com.johnsnowlabs.nlp.AnnotatorModel.transform(AnnotatorModel.scala:129)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "res = p_model.transform(spark.createDataFrame(sample_text, StringType()).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1ed2e80-f487-4fb2-8749-ccc1d924c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|In March 2012, as...|[{document, 0, 13...|[{document, 0, 13...|[{token, 0, 1, In...|\n",
      "|In February 2017,...|[{document, 0, 88...|[{document, 0, 88...|[{token, 0, 1, In...|\n",
      "|While our gross p...|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Wh...|\n",
      "|We reported an op...|[{document, 0, 12...|[{document, 0, 12...|[{token, 0, 1, We...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fb2db-1cee-4f78-a486-dd6c9f6abd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|chunk             |ner_label          |\n",
      "+------------------+-------------------+\n",
      "|March 2012        |DATE               |\n",
      "|Vertro            |ORG                |\n",
      "|ALOT              |PRODUCT            |\n",
      "|February 2017     |DATE               |\n",
      "|NetSeer           |ORG                |\n",
      "|81.4%             |PROFIT_INCREASE    |\n",
      "|27%               |REVENUES_DECLINED  |\n",
      "|$8,048,581 million|OPERATING_LOSS_2020|\n",
      "|$7,738,193        |OPERATING_LOSS_2019|\n",
      "|2019              |DATE               |\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "res.select(F.explode(F.arrays_zip(res.ner_chunk.result, res.ner_chunk.begin, res.ner_chunk.end, res.ner_chunk.metadata)).alias(\"cols\")) \\\n",
    "   .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "           F.expr(\"cols['3']['entity']\").alias(\"ner_label\"))\\\n",
    "   .filter(\"ner_label!='O'\")\\\n",
    "   .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
