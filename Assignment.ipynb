{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1025d109-114a-4733-a272-c00bf6c099f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in ./.local/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.local/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "# %pip install nltk\n",
    "# %pip install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cefc88-dbc1-4781-a8d2-8a210b81e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sparknlp\n",
    "# %pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69bc995f-e92d-4c08-85aa-ee9207b2b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findsparkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233aaa98-48cd-46ad-879e-0862d84787bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp==3.3.0\n",
      "  Downloading spark_nlp-3.3.0-py2.py3-none-any.whl (120 kB)\n",
      "     ---------------------------------------- 0.0/120.6 kB ? eta -:--:--\n",
      "     ----------------------------- --------- 92.2/120.6 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 120.6/120.6 kB 1.8 MB/s eta 0:00:00\n",
      "Installing collected packages: spark-nlp\n",
      "  Attempting uninstall: spark-nlp\n",
      "    Found existing installation: spark-nlp 5.1.4\n",
      "    Uninstalling spark-nlp-5.1.4:\n",
      "      Successfully uninstalled spark-nlp-5.1.4\n",
      "Successfully installed spark-nlp-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install spark-nlp==3.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a802c7c1-920c-411f-a9a9-9857d46b8200",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spark-nlp in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (3.3.0)\n",
      "Collecting spark-nlp\n",
      "  Using cached spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "  Attempting uninstall: spark-nlp\n",
      "    Found existing installation: spark-nlp 3.3.0\n",
      "    Uninstalling spark-nlp-3.3.0:\n",
      "      Successfully uninstalled spark-nlp-3.3.0\n",
      "Successfully installed spark-nlp-5.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install spark-nlp --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96cdb33c-5a87-4dbe-b060-6d8312c242b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060639c-9f56-47d2-ad0d-9f784713c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57d61b-55e8-4ce8-91e4-73520c8c25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6bb00344-1c2c-4888-9d66-33df2c72646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def isplural(word):\n",
    "    lemma = wnl.lemmatize(word, 'n')\n",
    "    plural = True if word is not lemma else False\n",
    "    return plural, lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee26eec6-51b3-4bf8-8219-d1999d287cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('furniture stores pages.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    urls = lines[1:]\n",
    "\n",
    "urls = list(map(lambda x : re.sub(r'\\n', '', x), urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b302df3-84c3-4b25-94ad-41d995b01d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 5.62 s, total: 2min 57s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def scrape_product_info(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # Extract product information based on the HTML structure of the websites\n",
    "        try:\n",
    "            title = soup.find('title').text.strip()\n",
    "        except:\n",
    "            title = \"\"\n",
    "        try:\n",
    "            description = soup.find('meta', {'name': 'description'})['content'].strip()\n",
    "        except:\n",
    "            description = \"\"\n",
    "\n",
    "        response = {'title': title, 'description': description, 'url': url}\n",
    "    except:\n",
    "        response = {}\n",
    "\n",
    "    # You can add more logic to extract other relevant information\n",
    "\n",
    "    return response\n",
    "\n",
    "def scrape_product_info_parallel(urls, max_workers=None):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Use map to apply the square_number function to each element of the list\n",
    "        squared_numbers = list(executor.map(scrape_product_info, urls))\n",
    "    return squared_numbers\n",
    "\n",
    "product_info = scrape_product_info_parallel(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93103254-7e82-468b-b618-c86da080e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [v.get('title', '') + v.get('description', '') for v in product_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056d3941-0375-424d-8a22-97526652e2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Factory Buys 32cm Euro Top Mattress - KingPamper yourself with Factory Buys Euro Top Mattress that will give you a soothing sleep night after night.Incorporating the cutting-edge technology, the lavish construction addresses all elements that will offer you a comfortable and luxurious experience - Euro top padding with high-density foam and an independent coil',\n",
       " 'Beadlight Cirrus LED Reading Light\\n– Dunlin HomeBeadlight wall light, LED reading light, First call lights, reading light first class, plane reading lights, reading light led, bed light, beadhead light, led',\n",
       " 'Hamar Plant Stand - Ash\\n– The ModernThe Hamar Plant Stand is a wonderful accessory that holds at least three pot plants or other objects such as vases, sculptural objects or lamps. Use it to fill a corner or that tricky spot, bring height and greenery to your room.\\xa0\\xa0 Dimensions: 65cm at widest point x 60cm (D) x 92cm (H)Each platform has 28cm (W) x 28cm',\n",
       " '',\n",
       " '',\n",
       " '404 Not Found – Home-Buy Interiors',\n",
       " 'Gift Cards | RJ LivingShop Gift Cards Online at RJ Living. Enjoy fast Australia wide delivery with 7 day money back guarantee. Buy now, pay later with Afterpay or ZipPay.',\n",
       " '',\n",
       " 'Dining Tables | Living EdgeShop a range of dining tables to suit your home or workplace. Choose from solid timber, glass and marble tables.',\n",
       " '404 Not Found']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48c84e29-f29f-4fee-80c2-f09e8945ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words\n",
    "all_words = []\n",
    "for text in data:\n",
    "    words = word_tokenize(text.lower())  # Convert to lowercase to ensure case-insensitivity\n",
    "    all_words.extend(words)\n",
    "\n",
    "filtered_words = [word for word in all_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37c6b12a-f4b4-464a-b7fd-bc604cb5ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = pos_tag(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c31ac2b8-c19d-42a2-8619-9ea579d7abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [word for word, pos in pos_tags if pos.startswith('NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f48b599c-dee9-4bc5-97d4-8a528acd7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_nouns = [wnl.lemmatize(word, 'n') for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0cb283b2-bd64-47c6-91a3-e52fbc2cda50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Andrei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8d90e321-42ee-4cbf-8684-2f1786c9a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = [word.lower() for word in lemmatized_nouns if word.lower() in english_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8c8ee03a-b430-4bed-b9c7-323586dec5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words, counts = np.unique(english_words, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d4c3cc42-5c02-4743-ad86-ec8d1f85f8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_frequency = {k: v for (k,v) in sorted(list(zip(unique_words, counts)), key=lambda item: item[1], reverse=True) if len(k)>2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "32908e5e-2826-4be4-9bb9-814dabbd112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7ddf9db-f52f-4eea-bce1-6a95dae88909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the list to the CSV file\n",
    "with open('annotated_words.csv', \"w\", newline=\"\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow([\"Words\"])  # Write header\n",
    "    csv_writer.writerows([[element] for element in sorted_word_frequency.keys()])  # Write elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc08a6e-3ad9-4922-8b10-c0547364a7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  5.1.4\n",
      "Apache Spark version:  3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import sparknlp\n",
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9786225b-e450-4957-96be-aa1fc671c604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp. training import CoNLL\n",
    "from pyspark.ml import Pipeline\n",
    "spark = sparknlp.start()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel, Lemmatizer, StopWordsCleaner\n",
    "from sparknlp import DocumentAssembler\n",
    "from sparknlp.base import Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77dda2-8bbd-4d1e-8f9c-566664d5a1e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data = CoNLL().readDataset(spark, './eng.train' ) # CONLL2003 train set\n",
    "\n",
    "bert = BertEmbeddings.pretrained('bert_base_cased', 'en')\\\n",
    "        .setInputCols([\"sentence\", 'token'])\\\n",
    "        .setOutputCol(\"bert\")\\\n",
    "\n",
    "# Trains Tensorflow based Char-CNN-BLSTM model\n",
    "nerTagger = NerDLApproach()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
    "    .setLabelColumn( \"label\" ) \\\n",
    "    .setOutputCol(\"ner\")\\\n",
    "    .setMaxEpochs(1)\n",
    "\n",
    "pipeline = Pipeline(stages = [bert, nerTagger])\n",
    "nermodel = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae746bd-540f-4ef2-b8c6-f76f1801eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a0d0b5-57d0-46f6-8c75-6579479ab9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel, Lemmatizer, StopWordsCleaner\n",
    "\n",
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48634e78-7694-4932-a3aa-d5be191b7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71ae4413-2f4b-40eb-8486-7b3856e87624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP Tokenizer Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1\", \"Spark NLP is an awesome library. Natural language processing made easy with Spark.\"),\n",
    "               # 'B-PROD, I-PROD, O, O, B-FEAT, B-PROD, O, B-PROD, I-PROD, I-PROD, O, O, O, B-PROD, O'),\n",
    "        (\"2\", \"Natural language processing made easy with Spark. Spark NLP is an awesome library.\"),\n",
    "                # 'B-PROD, I-PROD, I-PROD, O, O, O, B-PROD, O, B-PROD, I-PROD, O, O, B-FEAT, B-PROD, O')\n",
    "       ]\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"id\", \"text\"] #, \"label\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2256e1-52f8-4bed-a2ba-9d62b153a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "finisher = Finisher().setInputCols([\"token\"]).setOutputCols([\"tokens\"]).setOutputAsArray(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c4a464-6b25-40a4-8a71-9b1d4e650e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the pipeline\n",
    "pipeline = [document_assembler, tokenizer, finisher]\n",
    "pipeline = Pipeline().setStages(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cadd34e-77b5-4019-9141-b3b3963dc5a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fit and transform the DataFrame\n",
    "result = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175c8cd2-acd3-4db1-9497-1e97c5a5af7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------------------------+\n",
      "|id |tokens                                                                                              |\n",
      "+---+----------------------------------------------------------------------------------------------------+\n",
      "|1  |[Spark, NLP, is, an, awesome, library, ., Natural, language, processing, made, easy, with, Spark, .]|\n",
      "|2  |[Natural, language, processing, made, easy, with, Spark, ., Spark, NLP, is, an, awesome, library, .]|\n",
      "+---+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "result.select(\"id\", \"tokens\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4f9d527-b034-40fe-bca7-31491eafd38d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "pos_tagger = PerceptronModel.pretrained().setInputCols([\"document\", \"token\"]).setOutputCol(\"pos\")\n",
    "finisher = Finisher().setInputCols([\"pos\"]).setOutputCols([\"pos_tags\"]).setOutputAsArray(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66d8a067-b14d-4aef-86b5-705b16f2740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the pipeline\n",
    "pos_pipeline = [document_assembler, tokenizer, pos_tagger, finisher]\n",
    "pos_pipeline = Pipeline().setStages(pos_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b80db059-f14f-4544-a107-b8df6f78dd97",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|id |text                                                                              |pos_tags                                                        |\n",
      "+---+----------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|1  |Spark NLP is an awesome library. Natural language processing made easy with Spark.|[NNP, NNP, VBZ, DT, JJ, NN, ., NNP, NN, NN, VBN, JJ, IN, NNP, .]|\n",
      "|2  |Natural language processing made easy with Spark. Spark NLP is an awesome library.|[NNP, NN, NN, VBN, JJ, IN, NNP, ., NNP, NNP, VBZ, DT, JJ, NN, .]|\n",
      "+---+----------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the DataFrame\n",
    "result = pos_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Show the result\n",
    "result.select(\"id\", \"text\", \"pos_tags\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb9f465f-c432-481d-907c-d3c85f133e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "\n",
    "from sparknlp.annotator import (\n",
    "    NerCrfApproach, \n",
    "    WordEmbeddingsModel,\n",
    "    SentenceDetector,\n",
    "    Tokenizer,\n",
    "    PerceptronModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55b34b27-146d-414d-973b-4bf914150cd6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NerDLApproach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1625021b-43f1-44ac-b7cc-b7e11a40bc1a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Define the pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol(\"text\") \\\n",
    "                        .setOutputCol(\"document\")\n",
    "sentence_detector = SentenceDetector() \\\n",
    "                        .setInputCols(\"document\") \\\n",
    "                        .setOutputCol(\"sentence\")\n",
    "tokenizer = Tokenizer() \\\n",
    "                        .setInputCols([\"document\"]) \\\n",
    "                        .setOutputCol(\"token\")\n",
    "pos_tagger = PerceptronModel.pretrained() \\\n",
    "                        .setInputCols(\"sentence\", \"token\") \\\n",
    "                        .setOutputCol(\"pos\")\n",
    "embeddings = WordEmbeddingsModel.pretrained() \\\n",
    "                        .setInputCols(\"sentence\", \"token\") \\\n",
    "                        .setOutputCol(\"embeddings\") \\\n",
    "                        .setCaseSensitive(False)\n",
    "ner_tagger = NerCrfApproach() \\\n",
    "                        .setInputCols(\"sentence\", \"token\", \"pos\", \"embeddings\") \\\n",
    "                        .setLabelColumn(\"label\") \\\n",
    "                        .setOutputCol(\"ner\") \\\n",
    "                        .setMinEpochs(1) \\\n",
    "                        .setMaxEpochs(5) \\\n",
    "                        .setLossEps(1e-3)\n",
    "\n",
    "finisher = Finisher().setInputCols([\"ner\"]).setOutputCols([\"ner_tags\"]).setOutputAsArray(True)\n",
    "\n",
    "# Assemble the pipeline\n",
    "ner_pipeline = [document_assembler, \n",
    "                sentence_detector, \n",
    "                tokenizer,\n",
    "                pos_tagger,\n",
    "                embeddings,\n",
    "                ner_tagger,\n",
    "                finisher\n",
    "               ]\n",
    "\n",
    "ner_pipeline = Pipeline().setStages(ner_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a8c3c75-52fb-47ad-a140-c4d7e4d59882",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1344.fit.\n: java.lang.ClassCastException: class java.lang.String cannot be cast to class scala.collection.TraversableLike (java.lang.String is in module java.base of loader 'bootstrap'; scala.collection.TraversableLike is in unnamed module of loader 'app')\r\n\tat com.johnsnowlabs.nlp.annotators.common.Tagged.getAnnotations(Tagged.scala:117)\r\n\tat com.johnsnowlabs.nlp.annotators.common.Tagged.getAnnotations$(Tagged.scala:116)\r\n\tat com.johnsnowlabs.nlp.annotators.common.NerTagged$.getAnnotations(Tagged.scala:165)\r\n\tat com.johnsnowlabs.nlp.annotators.common.NerTagged$.$anonfun$collectTrainingInstancesWithPos$1(Tagged.scala:180)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat com.johnsnowlabs.nlp.annotators.common.NerTagged$.collectTrainingInstancesWithPos(Tagged.scala:179)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfApproach.train(NerCrfApproach.scala:325)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfApproach.train(NerCrfApproach.scala:135)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:69)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:75)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit and transform the DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mner_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df)\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1344.fit.\n: java.lang.ClassCastException: class java.lang.String cannot be cast to class scala.collection.TraversableLike (java.lang.String is in module java.base of loader 'bootstrap'; scala.collection.TraversableLike is in unnamed module of loader 'app')\r\n\tat com.johnsnowlabs.nlp.annotators.common.Tagged.getAnnotations(Tagged.scala:117)\r\n\tat com.johnsnowlabs.nlp.annotators.common.Tagged.getAnnotations$(Tagged.scala:116)\r\n\tat com.johnsnowlabs.nlp.annotators.common.NerTagged$.getAnnotations(Tagged.scala:165)\r\n\tat com.johnsnowlabs.nlp.annotators.common.NerTagged$.$anonfun$collectTrainingInstancesWithPos$1(Tagged.scala:180)\r\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n\tat com.johnsnowlabs.nlp.annotators.common.NerTagged$.collectTrainingInstancesWithPos(Tagged.scala:179)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfApproach.train(NerCrfApproach.scala:325)\r\n\tat com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfApproach.train(NerCrfApproach.scala:135)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach._fit(AnnotatorApproach.scala:69)\r\n\tat com.johnsnowlabs.nlp.AnnotatorApproach.fit(AnnotatorApproach.scala:75)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform the DataFrame\n",
    "result = ner_pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f68ee-3c45-4a21-94a3-2268d0326a57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Show the result\n",
    "result.select(\"id\", \"text\", \"ner_tags\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15f5e13e-19c4-420e-a87f-a0f534d0e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c266027-02c3-4e19-b7ac-b54a9ab78bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_6B_300 download started this may take some time.\n",
      "Approximate size to download 426.2 MB\n",
      "[OK!]\n",
      "ner_dl download started this may take some time.\n",
      "Approximate size to download 13.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols('document') \\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "'''\n",
    "Note:\n",
    "\n",
    "For embeddings that offer multi-language support like glove_6B_300, we use 'xx'\n",
    "in the language parameter.\n",
    "\n",
    "e.g: embeddings = WordEmbeddingsModel.pretrained('glove_6B_300', lang='xx') \\\n",
    "        .setInputCols([\"sentence\", 'token']) \\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "For dedicated embeddings like 'aner_cc_300d' the language parameter would be \n",
    "the corresponding language.\n",
    "\n",
    "e.g: embeddings = WordEmbeddingsModel.pretrained('aner_cc_300d', lang='ar') \\\n",
    "        .setInputCols([\"sentence\", 'token']) \\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "'''\n",
    "## to use Glove embeddings\n",
    "embeddings = WordEmbeddingsModel.pretrained('glove_6B_300', lang='xx') \\\n",
    "        .setInputCols([\"sentence\", 'token']) \\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "## to use Bert Embeddings\n",
    "#embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
    "#        .setInputCols(['document', 'token']) \\\n",
    "#        .setOutputCol('embeddings')\n",
    "\n",
    "# select your desired model and define language\n",
    "ner_model = NerDLModel.pretrained() \\\n",
    "    .setInputCols(['sentence', 'token', 'embeddings']) \\\n",
    "    .setOutputCol('ner')\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols(['sentence', 'token', 'ner']) \\\n",
    "    .setOutputCol('ner_chunk')\n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler, \n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner_model,\n",
    "        ner_converter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1f1d52c-013e-436e-96fa-83d8b7bdb805",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Found input column with storage metadata. But such ref does not match to the ref this annotator requires. Make sure you are loading the annotator with ref: glove_100d",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(data, StringType())\u001b[38;5;241m.\u001b[39mtoDF(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\pipeline.py:304\u001b[0m, in \u001b[0;36mPipelineModel._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages:\n\u001b[1;32m--> 304\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Found input column with storage metadata. But such ref does not match to the ref this annotator requires. Make sure you are loading the annotator with ref: glove_100d"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, StringType()).toDF(\"text\")\n",
    "result = nlp_pipeline.fit(df).transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
