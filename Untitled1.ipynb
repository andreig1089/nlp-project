{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a160b01-1823-4a5c-bca2-1b9799566253",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "     ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/7.9 MB 991.0 kB/s eta 0:00:08\n",
      "      --------------------------------------- 0.1/7.9 MB 1.7 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.3/7.9 MB 2.1 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.6/7.9 MB 3.1 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.8/7.9 MB 3.5 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.1/7.9 MB 4.0 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.4/7.9 MB 4.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.6/7.9 MB 4.1 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 1.9/7.9 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.4/7.9 MB 4.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 2.7/7.9 MB 5.0 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 3.1/7.9 MB 5.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.5/7.9 MB 5.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 3.8/7.9 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 4.1/7.9 MB 5.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.5/7.9 MB 5.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 4.6/7.9 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 5.0/7.9 MB 5.7 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.4/7.9 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 6.0/7.9 MB 6.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.4/7.9 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 6.9/7.9 MB 6.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 7.3/7.9 MB 6.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 7.7/7.9 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.9/7.9 MB 6.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.9/7.9 MB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "     ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 311.7/311.7 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.4/2.2 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.9/2.2 MB 9.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.3/2.2 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.6/2.2 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp310-none-win_amd64.whl (277 kB)\n",
      "     ---------------------------------------- 0.0/277.3 kB ? eta -:--:--\n",
      "     -------------------------------------  276.5/277.3 kB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 277.3/277.3 kB 8.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 166.4/166.4 kB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrei\\documents\\dev\\veridion_assignment\\veridion_venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Installing collected packages: safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.19.4 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543afdf8-10ce-4c6c-82c3-6fe009660487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "\n",
    "label_list = ['O','B-MISC','I-MISC','B-PER','I-PER','B-ORG','I-ORG','B-LOC','I-LOC']\n",
    "label_encoding_dict = {'I-PRG': 2,'I-I-MISC': 2, 'I-OR': 6, 'O': 0, 'I-': 0, 'VMISC': 0, 'B-PER': 3, 'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8, 'B-MISC': 1, 'I-MISC': 2}\n",
    "\n",
    "task = \"ner\" \n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def get_all_tokens_and_ner_tags(directory):\n",
    "    return pd.concat([get_tokens_and_ner_tags(os.path.join(directory, filename)) for filename in os.listdir(directory)]).reset_index().drop('index', axis=1)\n",
    "    \n",
    "def get_tokens_and_ner_tags(filename):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        split_list = [list(y) for x, y in itertools.groupby(lines, lambda z: z == '\\n') if not x]\n",
    "        tokens = [[x.split('\\t')[0] for x in y] for y in split_list]\n",
    "        entities = [[x.split('\\t')[1][:-1] for x in y] for y in split_list] \n",
    "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n",
    "  \n",
    "def get_un_token_dataset(train_directory, test_directory):\n",
    "    train_df = get_all_tokens_and_ner_tags(train_directory)\n",
    "    test_df = get_all_tokens_and_ner_tags(test_directory)\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    return (train_dataset, test_dataset)\n",
    "\n",
    "train_dataset, test_dataset = get_un_token_dataset('./UN-named-entity-recognition/tagged-training/', './UN-named-entity-recognition/tagged-test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a6a53-cb0a-4e13-874a-18e4ea2502fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    label_all_tokens = True\n",
    "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif label[word_idx] == '0':\n",
    "                label_ids.append(0)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "        \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
